{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.0-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /Users/mehul/causaLM-for-clinical-notes-interpretability/.conda/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mehul/causaLM-for-clinical-notes-interpretability/.conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec in /Users/mehul/causaLM-for-clinical-notes-interpretability/.conda/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.5.0-cp310-none-macosx_11_0_arm64.whl (64.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/causaLM-for-clinical-notes-interpretability/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Load the clinical notes dataset\n",
    "df = pd.read_csv('../data/release_train_patients.csv')  # Replace with your actual file path\n",
    "\n",
    "# Load the evidence mapping dataset\n",
    "with open('../data/release_evidences.json', 'r') as f:\n",
    "    evidence_map = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_note_for_patient(evidences_str, evidence_map):\n",
    "    if not evidences_str or pd.isna(evidences_str):\n",
    "        return ''\n",
    "    try:\n",
    "        # Convert the string representation of the list into an actual list\n",
    "        evidences = ast.literal_eval(evidences_str)\n",
    "        if not isinstance(evidences, list):\n",
    "            return ''\n",
    "        note_parts = []\n",
    "        for evidence in evidences:\n",
    "            # Evidence codes might be in the format 'E_55_@_V_89'\n",
    "            if '_@_' in evidence:\n",
    "                e_code, v_code = evidence.split('_@_')\n",
    "            else:\n",
    "                e_code = evidence\n",
    "                v_code = None\n",
    "            mapping = evidence_map.get(e_code, {})\n",
    "            question = mapping.get('question_en', '').strip()\n",
    "            if not question:\n",
    "                continue  # Skip if question is not available\n",
    "            if v_code and 'value_meaning' in mapping and v_code in mapping['value_meaning']:\n",
    "                answer = mapping['value_meaning'][v_code]['en'].strip()\n",
    "            else:\n",
    "                # Use default value or indicate missing answer\n",
    "                default_value = mapping.get('default_value', '')\n",
    "                if default_value and 'value_meaning' in mapping and default_value in mapping['value_meaning']:\n",
    "                    answer = mapping['value_meaning'][default_value]['en'].strip()\n",
    "                else:\n",
    "                    answer = ''  # Missing answer\n",
    "            # Construct the question-answer pair\n",
    "            if answer:\n",
    "                note_parts.append(f\"Q: {question} A: {answer}\")\n",
    "\n",
    "        # Combine all question-answer pairs to form the note\n",
    "        note = '. '.join(note_parts)\n",
    "        return note\n",
    "    except (ValueError, SyntaxError):\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chest_pain(evidences_str, chest_pain_evidences):\n",
    "    if not evidences_str or pd.isna(evidences_str):\n",
    "        return 0\n",
    "    try:\n",
    "        # Convert the string representation of the list into an actual list\n",
    "        evidences = ast.literal_eval(evidences_str)\n",
    "        if not isinstance(evidences, list):\n",
    "            return 0\n",
    "        # Check if any evidence code indicates chest pain\n",
    "        for evidence in evidences:\n",
    "            if evidence in chest_pain_evidences:\n",
    "                return 1\n",
    "        return 0\n",
    "    except (ValueError, SyntaxError):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['note'] = df['EVIDENCES'].apply(lambda x: generate_note_for_patient(x, evidence_map))\n",
    "\n",
    "df['unique_id'] = df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9230418\n"
     ]
    }
   ],
   "source": [
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chest_pain_evidences(evidence_map):\n",
    "    chest_pain_evidences = set()\n",
    "    # Keywords to identify chest pain\n",
    "    chest_keywords = ['chest', 'sternum', 'thorax', 'breast', 'pectoral', 'rib', 'precordial']\n",
    "    pain_keywords = ['pain', 'douleur']\n",
    "    \n",
    "    for e_code, mapping in evidence_map.items():\n",
    "        question_en = mapping.get('question_en', '').lower()\n",
    "        question_fr = mapping.get('question_fr', '').lower()\n",
    "        \n",
    "        # Check if the question is about pain\n",
    "        if any(pain_kw in question_en for pain_kw in pain_keywords) or \\\n",
    "           any(pain_kw in question_fr for pain_kw in pain_keywords):\n",
    "            # Check if there are value meanings\n",
    "            value_meaning = mapping.get('value_meaning', {})\n",
    "            for v_code, meaning in value_meaning.items():\n",
    "                meaning_en = meaning.get('en', '').lower()\n",
    "                # Check if the meaning indicates chest area\n",
    "                if any(chest_kw in meaning_en for chest_kw in chest_keywords):\n",
    "                    chest_pain_evidences.add(f\"{e_code}_@_{v_code}\")\n",
    "    return chest_pain_evidences\n",
    "\n",
    "# Get the set of evidence codes that indicate chest pain\n",
    "chest_pain_evidences = get_chest_pain_evidences(evidence_map)\n",
    "\n",
    "\n",
    "def extract_chest_pain(evidences_str, chest_pain_evidences):\n",
    "    if not evidences_str or pd.isna(evidences_str):\n",
    "        return 0\n",
    "    try:\n",
    "        evidences = ast.literal_eval(evidences_str)\n",
    "        if not isinstance(evidences, list):\n",
    "            return 0\n",
    "        for evidence in evidences:\n",
    "            if evidence in chest_pain_evidences:\n",
    "                return 1\n",
    "        return 0\n",
    "    except (ValueError, SyntaxError):\n",
    "        return 0\n",
    "\n",
    "# Apply the function to create the 'chest_pain' column\n",
    "df['chest_pain'] = df['EVIDENCES'].apply(lambda x: extract_chest_pain(x, chest_pain_evidences))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/clinical_notes_with_chest_pain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalNotesCausalDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length, controlled_concepts=None, mask_prob=0.15):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with clinical notes.\n",
    "            tokenizer (BertTokenizer): Tokenizer for BERT.\n",
    "            max_length (int): Maximum sequence length.\n",
    "            controlled_concepts (list): List of controlled concept labels (optional).\n",
    "            mask_prob (float): Probability of masking tokens for MLM.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.controlled_concepts = controlled_concepts\n",
    "        self.mask_prob = mask_prob\n",
    "        \n",
    "        # Precompute all controlled concept labels if provided\n",
    "        if self.controlled_concepts:\n",
    "            self.controlled_concept_map = {concept: idx for idx, concept in enumerate(self.controlled_concepts)}\n",
    "        else:\n",
    "            self.controlled_concept_map = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        note = row['note']\n",
    "        chest_pain = row['chest_pain']\n",
    "        controlled_concept = row.get('controlled_concept', None)  # Assuming you have this column\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            note,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Prepare MLM labels\n",
    "        labels = input_ids.clone()\n",
    "        # Create mask\n",
    "        probability_matrix = torch.full(labels.shape, self.mask_prob)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.unsqueeze(0).tolist()\n",
    "        ]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "        # Replace masked input tokens with [MASK] token\n",
    "        input_ids[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        \n",
    "        # Treated Concept Label\n",
    "        chest_pain_label = torch.tensor(chest_pain, dtype=torch.long)\n",
    "        \n",
    "        # Controlled Concept Label (if any)\n",
    "        if self.controlled_concepts and controlled_concept:\n",
    "            cc_label = torch.tensor(self.controlled_concept_map.get(controlled_concept, 0), dtype=torch.long)\n",
    "        else:\n",
    "            cc_label = torch.tensor(0, dtype=torch.long)  # Default to 0 if no controlled concept\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'chest_pain': chest_pain_label,\n",
    "            'controlled_concept': cc_label\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
