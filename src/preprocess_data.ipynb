{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the clinical notes dataset\n",
    "df = pd.read_csv('../data/release_train_patients.csv')  # Replace with your actual file path\n",
    "\n",
    "# Load the evidence mapping dataset\n",
    "with open('../data/release_evidences.json', 'r') as f:\n",
    "    evidence_map = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_note_for_patient(evidences_str, evidence_map):\n",
    "    if not evidences_str or pd.isna(evidences_str):\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        evidences = ast.literal_eval(evidences_str)\n",
    "        if not isinstance(evidences, list):\n",
    "            return ''\n",
    "        \n",
    "        grouped_evidences = defaultdict(lambda: {'question': '', 'answers': set()})\n",
    "        \n",
    "        for evidence in evidences:\n",
    "            base_code, v_code = evidence.rsplit('@', 1) if '@' in evidence else (evidence, None)\n",
    "            if v_code:\n",
    "                base_code = base_code[:-1]\n",
    "                v_code = v_code[1:]\n",
    "            \n",
    "            mapping = evidence_map.get(base_code, {})\n",
    "            question = mapping.get('question_en', '')\n",
    "            data_type = mapping.get('data_type')\n",
    "\n",
    "            if not question:\n",
    "                continue\n",
    "\n",
    "            grouped_evidences[base_code]['question'] = question\n",
    "\n",
    "            if data_type == \"B\":\n",
    "                answer = \"yes\"\n",
    "            elif data_type == \"C\":\n",
    "                value_meaning = mapping.get('value_meaning', {})\n",
    "                answer = value_meaning.get(v_code, v_code)\n",
    "                if answer != v_code:\n",
    "                    answer = answer.get('en', '').lower()\n",
    "            else:\n",
    "                value_meaning = mapping.get('value_meaning', {})\n",
    "                answer = value_meaning.get(v_code, {}).get('en', '').lower()\n",
    "\n",
    "            if answer:\n",
    "                grouped_evidences[base_code]['answers'].add(answer)\n",
    "        \n",
    "        return ' '.join(f\"Q: {details['question']} A: {', '.join(sorted(details['answers'])).capitalize()}.\"\n",
    "                        for details in grouped_evidences.values())\n",
    "    \n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error parsing evidences_str: {e}\")\n",
    "        return ''\n",
    "    \n",
    "def patient_basic_details(row, evidence_map=evidence_map):\n",
    "    output = []\n",
    "    age, sex, pathology, initial_evidence = row.get(\"AGE\"), row.get(\"SEX\"), row.get(\"PATHOLOGY\"), row.get(\"INITIAL_EVIDENCE\")\n",
    "\n",
    "    if age:\n",
    "        output.append(\"Q: What is your age? A: \" + str(age) + \".\")\n",
    "    if sex:\n",
    "        output.append(\"Q: What is your sex? A: \" + (\"Female.\" if sex == \"F\" else \"Male.\"))\n",
    "    if pathology:\n",
    "        output.append(\"Q: What is your pathology? A: \" + pathology + \".\")\n",
    "    if initial_evidence:\n",
    "        output.append(f'Q: {evidence_map.get(initial_evidence).get(\"question_en\")} A: Yes.')\n",
    "    \n",
    "    return \" \".join(output)\n",
    "\n",
    "df[\"basic_details\"] = df.apply(lambda x: patient_basic_details(x), axis=1)\n",
    "df[\"evidence_notes\"] = df[\"EVIDENCES\"].apply(lambda x: generate_note_for_patient(x, evidence_map))\n",
    "\n",
    "df[\"notes\"] = df[\"basic_details\"] + \" \" + df[\"evidence_notes\"]\n",
    "df.drop([\"AGE\",\"DIFFERENTIAL_DIAGNOSIS\",\"SEX\",\"PATHOLOGY\",\"EVIDENCES\",\"INITIAL_EVIDENCE\", \"basic_details\", \"evidence_notes\"], axis=1, inplace=True)\n",
    "df.to_csv('../data/clinical_notes_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/release_conditions.json', 'r', encoding='utf-8') as f:\n",
    "    disease_mapping = json.load(f)\n",
    "\n",
    "notes = pd.read_csv('../data/clinical_notes_preprocessed.csv')\n",
    "\n",
    "# Extract all disease names\n",
    "all_diseases = sorted([details['condition_name'] for details in disease_mapping.values()])\n",
    "\n",
    "# Create a mapping from disease to index\n",
    "disease_to_index = {disease: idx for idx, disease in enumerate(all_diseases)}\n",
    "\n",
    "# Handles Unknown Diseases\n",
    "disease_to_index['Other'] = 49\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Acute COPD exacerbation / infection': 0, 'Acute dystonic reactions': 1, 'Acute laryngitis': 2, 'Acute otitis media': 3, 'Acute pulmonary edema': 4, 'Acute rhinosinusitis': 5, 'Allergic sinusitis': 6, 'Anaphylaxis': 7, 'Anemia': 8, 'Atrial fibrillation': 9, 'Boerhaave': 10, 'Bronchiectasis': 11, 'Bronchiolitis': 12, 'Bronchitis': 13, 'Bronchospasm / acute asthma exacerbation': 14, 'Chagas': 15, 'Chronic rhinosinusitis': 16, 'Cluster headache': 17, 'Croup': 18, 'Ebola': 19, 'Epiglottitis': 20, 'GERD': 21, 'Guillain-Barré syndrome': 22, 'HIV (initial infection)': 23, 'Influenza': 24, 'Inguinal hernia': 25, 'Larygospasm': 26, 'Localized edema': 27, 'Myasthenia gravis': 28, 'Myocarditis': 29, 'PSVT': 30, 'Pancreatic neoplasm': 31, 'Panic attack': 32, 'Pericarditis': 33, 'Pneumonia': 34, 'Possible NSTEMI / STEMI': 35, 'Pulmonary embolism': 36, 'Pulmonary neoplasm': 37, 'SLE': 38, 'Sarcoidosis': 39, 'Scombroid food poisoning': 40, 'Spontaneous pneumothorax': 41, 'Spontaneous rib fracture': 42, 'Stable angina': 43, 'Tuberculosis': 44, 'URTI': 45, 'Unstable angina': 46, 'Viral pharyngitis': 47, 'Whooping cough': 48, 'Other': 49}\n"
     ]
    }
   ],
   "source": [
    "#print(disease_to_index)\n",
    "# {'Acute COPD exacerbation / infection': 0, 'Acute dystonic reactions': 1, 'Acute laryngitis': 2, 'Acute otitis media': 3, 'Acute pulmonary edema': 4, 'Acute rhinosinusitis': 5, 'Allergic sinusitis': 6, 'Anaphylaxis': 7, 'Anemia': 8, 'Atrial fibrillation': 9, 'Boerhaave': 10, 'Bronchiectasis': 11, 'Bronchiolitis': 12, 'Bronchitis': 13, 'Bronchospasm / acute asthma exacerbation': 14, 'Chagas': 15, 'Chronic rhinosinusitis': 16, 'Cluster headache': 17, 'Croup': 18, 'Ebola': 19, 'Epiglottitis': 20, 'GERD': 21, 'Guillain-Barré syndrome': 22, 'HIV (initial infection)': 23, 'Influenza': 24, 'Inguinal hernia': 25, 'Larygospasm': 26, 'Localized edema': 27, 'Myasthenia gravis': 28, 'Myocarditis': 29, 'PSVT': 30, 'Pancreatic neoplasm': 31, 'Panic attack': 32, 'Pericarditis': 33, 'Pneumonia': 34, 'Possible NSTEMI / STEMI': 35, 'Pulmonary embolism': 36, 'Pulmonary neoplasm': 37, 'SLE': 38, 'Sarcoidosis': 39, 'Scombroid food poisoning': 40, 'Spontaneous pneumothorax': 41, 'Spontaneous rib fracture': 42, 'Stable angina': 43, 'Tuberculosis': 44, 'URTI': 45, 'Unstable angina': 46, 'Viral pharyngitis': 47, 'Whooping cough': 48, 'Other': 49}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'X': notes[\"notes\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/causaLM-for-clinical-notes-interpretability/.conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y_vector'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_vector\n\u001b[1;32m     21\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIFFERENTIAL_DIAGNOSIS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m y: parse_y(y, disease_to_index, \u001b[38;5;28mlen\u001b[39m(all_diseases)))\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_vector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/causaLM-for-clinical-notes-interpretability/.conda/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/causaLM-for-clinical-notes-interpretability/.conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y_vector'"
     ]
    }
   ],
   "source": [
    "# Function to parse Y and create fixed-length vector\n",
    "def parse_y(y_str, disease_to_index, num_diseases):\n",
    "    # Safely evaluate the string to a Python list\n",
    "    try:\n",
    "        y_list = ast.literal_eval(y_str)\n",
    "    except:\n",
    "        y_list = []\n",
    "\n",
    "    y_vector = np.zeros(num_diseases, dtype=np.float32)\n",
    "    for disease, prob in y_list:\n",
    "        if disease in disease_to_index:\n",
    "            idx = disease_to_index[disease]\n",
    "            y_vector[idx] = prob\n",
    "        else:\n",
    "            # Optionally handle unknown diseases\n",
    "            y_vector[-1] += prob\n",
    "    return y_vector\n",
    "\n",
    "data['Y'] = df[\"DIFFERENTIAL_DIAGNOSIS\"].apply(lambda y: parse_y(y, disease_to_index, len(all_diseases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chest-related terms\n",
    "chest_related_terms = [\n",
    "    'chest', 'lower chest', 'posterior chest wall(l)', 'posterior chest wall(r)', 'side of the chest(l)', 'side of the chest(r)'\n",
    "    # Add more terms as needed\n",
    "]\n",
    "\n",
    "# Compile regex patterns for efficiency\n",
    "pain_location_pattern = re.compile(\n",
    "    r'Q:\\s*Do you feel pain somewhere\\?\\s*A:\\s*([^\\.]+)\\.',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_chest_pain_label(x_text, chest_terms):\n",
    "    \"\"\"\n",
    "    Extracts a binary label indicating the presence of chest pain.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_text (str): The clinical note in Q&A format.\n",
    "    - chest_terms (list): List of chest-related terms to search for.\n",
    "    \n",
    "    Returns:\n",
    "    - int: 1 if chest pain is present, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the patient reports having pain\n",
    "    \n",
    "    pain_location_match = pain_location_pattern.search(x_text)\n",
    "    if pain_location_match:\n",
    "        locations = pain_location_match.group(1).lower()\n",
    "        for term in chest_terms:\n",
    "            # Use word boundaries to avoid partial matches\n",
    "            if re.search(r'\\b' + re.escape(term) + r'\\b', locations):\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "# Apply the function to create the chest_pain_label column\n",
    "data['TC'] = data['X'].apply(lambda x: extract_chest_pain_label(x, chest_related_terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalNotesCausalDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length, controlled_concepts=None, mask_prob=0.15):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with clinical notes.\n",
    "            tokenizer (BertTokenizer): Tokenizer for BERT.\n",
    "            max_length (int): Maximum sequence length.\n",
    "            controlled_concepts (list): List of controlled concept labels (optional).\n",
    "            mask_prob (float): Probability of masking tokens for MLM.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.controlled_concepts = controlled_concepts\n",
    "        self.mask_prob = mask_prob\n",
    "        \n",
    "        # Precompute all controlled concept labels if provided\n",
    "        if self.controlled_concepts:\n",
    "            self.controlled_concept_map = {concept: idx for idx, concept in enumerate(self.controlled_concepts)}\n",
    "        else:\n",
    "            self.controlled_concept_map = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        note = row['note']\n",
    "        chest_pain = row['chest_pain']\n",
    "        controlled_concept = row.get('controlled_concept', None)  # Assuming you have this column\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            note,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Prepare MLM labels\n",
    "        labels = input_ids.clone()\n",
    "        # Create mask\n",
    "        probability_matrix = torch.full(labels.shape, self.mask_prob)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.unsqueeze(0).tolist()\n",
    "        ]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "        # Replace masked input tokens with [MASK] token\n",
    "        input_ids[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        \n",
    "        # Treated Concept Label\n",
    "        chest_pain_label = torch.tensor(chest_pain, dtype=torch.long)\n",
    "        \n",
    "        # Controlled Concept Label (if any)\n",
    "        if self.controlled_concepts and controlled_concept:\n",
    "            cc_label = torch.tensor(self.controlled_concept_map.get(controlled_concept, 0), dtype=torch.long)\n",
    "        else:\n",
    "            cc_label = torch.tensor(0, dtype=torch.long)  # Default to 0 if no controlled concept\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'chest_pain': chest_pain_label,\n",
    "            'controlled_concept': cc_label\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
